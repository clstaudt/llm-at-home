{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORCA Mini "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cls/miniforge3/envs/in-house-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.77s/it]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 132/132 [00:00<00:00, 113kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hugging Face model_path\n",
    "model_path = 'psmathur/orca_mini_3b'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map='auto',\n",
    "    offload_folder=\"~/Downloads/orca_mini_3b_offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate text function\n",
    "def generate_text(system, instruction, input=None):\n",
    "    \n",
    "    if input:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### System:\\n{system}\\n\\n### User:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    tokens = torch.LongTensor(tokens).unsqueeze(0)\n",
    "    tokens = tokens.to('cpu')\n",
    "\n",
    "    instance = {'input_ids': tokens,'top_p': 1.0, 'temperature':0.7, 'generate_len': 1024, 'top_k': 50}\n",
    "\n",
    "    length = len(tokens[0])\n",
    "    with torch.no_grad():\n",
    "        rest = model.generate(\n",
    "            input_ids=tokens, \n",
    "            max_length=length+instance['generate_len'], \n",
    "            use_cache=True, \n",
    "            do_sample=True, \n",
    "            top_p=instance['top_p'],\n",
    "            temperature=instance['temperature'],\n",
    "            top_k=instance['top_k']\n",
    "        )    \n",
    "    output = rest[0][length:]\n",
    "    string = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    return f'[!] Response: {string}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cls/miniforge3/envs/in-house-llm/lib/python3.10/site-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Response: Dear Mr. Sam Altman,\n",
      "\n",
      "I am writing to you on behalf of OpenAI, the AI research organization that you co-founded. We are writing to request that you convert GPT-4, a private model developed by OpenAI, to an open source project.\n",
      "\n",
      "As you know, OpenAI is dedicated to advancing the field of artificial intelligence and making its technologies available to the public. By converting GPT-4 to an open source project, we believe that we can accelerate the development of AI tools and applications that are beneficial to society as a whole. This will also allow for greater transparency and accountability in the use of AI technology.\n",
      "\n",
      "We believe that GPT-4 has the potential to revolutionize many fields, from natural language processing to computer vision to robotics. By making this model available as an open source project, we can bring together a community of developers and researchers who can collaborate to improve its performance and usability. This will not only benefit OpenAI, but also the broader AI community.\n",
      "\n",
      "We understand that there are legal and technical challenges involved in converting a private model to an open source project, but we are confident that we can work through these issues. We are also committed to ensuring that any modifications or improvements to the GPT-4 model are subject to rigorous testing and quality control.\n",
      "\n",
      "Thank you for considering our request. We look forward to the opportunity to work with you and the broader AI community to advance the field of AI technology.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample Test Instruction Used by Youtuber Sam Witteveen https://www.youtube.com/@samwitteveenai\n",
    "system = 'You are an AI assistant that follows instruction extremely well. Help as much as you can.'\n",
    "instruction = 'Write a letter to Sam Altman, CEO of OpenAI, requesting him to convert GPT4 a private model by OpenAI to an open source project'\n",
    "print(generate_text(system, instruction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[!] Response: Dear Mr. Sam Altman,\n",
    "\n",
    "I am writing to you on behalf of OpenAI, the AI research organization that you co-founded. We are writing to request that you convert GPT-4, a private model developed by OpenAI, to an open source project.\n",
    "\n",
    "As you know, OpenAI is dedicated to advancing the field of artificial intelligence and making its technologies available to the public. By converting GPT-4 to an open source project, we believe that we can accelerate the development of AI tools and applications that are beneficial to society as a whole. This will also allow for greater transparency and accountability in the use of AI technology.\n",
    "\n",
    "We believe that GPT-4 has the potential to revolutionize many fields, from natural language processing to computer vision to robotics. By making this model available as an open source project, we can bring together a community of developers and researchers who can collaborate to improve its performance and usability. This will not only benefit OpenAI, but also the broader AI community.\n",
    "\n",
    "We understand that there are legal and technical challenges involved in converting a private model to an open source project, but we are confident that we can work through these issues. We are also committed to ensuring that any modifications or improvements to the GPT-4 model are subject to rigorous testing and quality control.\n",
    "\n",
    "Thank you for considering our request. We look forward to the opportunity to work with you and the broader AI community to advance the field of AI technology.\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "[Your Name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in-house-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
